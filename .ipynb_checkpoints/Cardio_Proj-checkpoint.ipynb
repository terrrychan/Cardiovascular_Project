{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## import the different modules ########\n",
    "\n",
    "# for matlab data\n",
    "import scipy.io as sio\n",
    "\n",
    "# signal processing module + CV \n",
    "from scipy import signal\n",
    "import cv2\n",
    "\n",
    "# array module \n",
    "import numpy as np\n",
    "\n",
    "# plotting module \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for html video \n",
    "# import io \n",
    "# import base64\n",
    "# from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## play as a html video ########\n",
    "\n",
    "video = io.open(r'C:\\Users\\Terence\\Dropbox\\breathing-A.MOV', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Print different ways to read in data ########\n",
    "\n",
    "temp = dir(cv2)\n",
    "#print(temp)\n",
    "type(temp)\n",
    "\n",
    "# for index in temp:\n",
    "#     string = index\n",
    "#     if string.startswith( 'COLOR' ):\n",
    "#         print(string)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(r'C:\\Users\\Terence\\Dropbox\\breathing-A.MOV')\n",
    "    cv2.imshow('frame', gray)\n",
    "\n",
    "### Print the properties of cap/cv2.VideoCapture\n",
    "# print(type(cap))\n",
    "# dir(cap)\n",
    "\n",
    "\n",
    "## Double check if this works! ##\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "     \n",
    "if int(major_ver)  < 3 :\n",
    "    fps = cap.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    print(\"1\")\n",
    "    print(format(fps))\n",
    "else :\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"2\")\n",
    "    length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    print(format(length))\n",
    "    print(format(fps))\n",
    "    actual_length = length/fps/60\n",
    "    print(actual_length)\n",
    "    print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "cap.release(); \n",
    "\n",
    "# CV_CAP_PROP_FRAME_WIDTH and FRAME_LENGTH and FRAME_HEIGHT\n",
    "# CV_CAP_PROP_FPS Frame rate.\n",
    "# CV_CAP_PROP_FOURCC 4-character code of codec.\n",
    "# CV_CAP_PROP_FRAME_COUNT Number of frames in the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected cv::KeyPoint for argument 'keypoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3eab8fb72588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mkp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawKeypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeypoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected cv::KeyPoint for argument 'keypoints'"
     ]
    }
   ],
   "source": [
    "######## Plays the video in a new window ######## \n",
    "# -- Documentation about xfeatures2d/surf/sift http://shimat.github.io/opencvsharp/html/1e9d5853-862c-1906-51fc-c1c53f6b4788.htm ##\n",
    "\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(r'/home/terence/cvproj/holding-A.MOV')\n",
    "counts = 0\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)#     cv2.imshow('frame',frame) ### imshow uses another window\n",
    "\n",
    "#     cv2.imshow('frame',frame) ### imshow uses another window\n",
    "    \n",
    "#     points = (50,100),(200,200)\n",
    "#     radius = 10\n",
    "#     color = (255, 0, 0)\n",
    "        \n",
    "#     # TODO: Try to get the text to display ## \n",
    "#     cv2.putText(gray, \"DRUGS\", color = (255,0,0))\n",
    "    \n",
    "#     sift = cv2.xfeatures2d.SIFT_create()\n",
    "#     kp = sift.detect(gray,None)\n",
    "\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    surf.setHessianThreshold(500)\n",
    "    kp = surf.detect(gray,None)\n",
    "\n",
    "#     look into FLANN and BF matcher \n",
    "    \n",
    "    gray = cv2.drawKeypoints(image = gray, keypoints = kp, outImage = gray, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imshow('frame', gray)\n",
    "\n",
    "#     for index in range(len(points)): # draw a circle... there migth be a built-in function already \n",
    "#         int1,int2 = points[index]\n",
    "#         cv2.circle(gray, (int1,incenter,radius, color, 3)\n",
    "                          \n",
    "#     gray_fft = np.fft.fft(gray)\n",
    "#     print('\\n')\n",
    "    \n",
    "#     size1 = gray.shape[0]  ### get the number of rows --- replace gray_fft with tracked features\n",
    "#     newarray = np.zeros((size1,1)) ###  initialize empty array\n",
    "#     for index in range(size1):\n",
    "#         gray_fft = np.fft.fft(gray[[index],:])\n",
    "#         newarray = np.sum([newarray,twoarray],axis = 1) # axis = 1 is adding the rows together \n",
    "#     final_fft = newarray / size1\n",
    "\n",
    "#     f_legend = camFR*(0:nFrames/2-1)/nFrames;\n",
    "#     break ### break to loop only once to test \n",
    "\n",
    "    counts += 1;\n",
    "    if counts > 1000:\n",
    "        break\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): ### Play around with waitkey(#) to play faster/slower\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function FlannBasedMatcher>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.FlannBasedMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terence\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:135: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300000, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Reading in the mat file ########\n",
    "# All variables stored in contents --- ignore the warning I think\n",
    "contents = sio.loadmat(r'C:\\Users\\Terence\\Dropbox\\breathing-A.mat') \n",
    "\n",
    "# specify certain variable through contents['var']\n",
    "data = contents['data']\n",
    "time = contents['t']\n",
    "\n",
    "## TODO: Add function to make sure shape is always the same \n",
    "time = np.transpose(time)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## MATLAB - style IIR filter design ########\n",
    "\n",
    "### Implement something like this to better list out frequencies?\n",
    "# N = 10 # N -- order\n",
    "# cutoff = 2 # Cutoff Freq\n",
    "# fs = 30000 # Sampling frequnecy\n",
    "# nyq = 0.5 * fs\n",
    "# Wn = cutoff / nyq\n",
    "\n",
    "N, Wn = signal.buttord(0.0005, 5, 3, 40, False) # user buttord to determine the minimum order of the filter \n",
    "b,a = signal.butter(N, Wn, 'low', analog= False) \n",
    "\n",
    "filtered_signal = signal.filtfilt(b,a, np.transpose(data)) # filtfilt has no phase distortion, better than lfilter?\n",
    "filtered_signal = np.transpose(filtered_signal)\n",
    "\n",
    "# plt.clf()\n",
    "# plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(time,data, color = 'silver',label = 'raw')\n",
    "plt.plot(time,filtered_signal, color = 'black', label = 'Original')\n",
    "\n",
    "# plot properties\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('ECG Magnitude [v]')\n",
    "\n",
    "plt.show()\n",
    "print(\"works!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
